import yaml, sys, time, json
import subprocess

def launch(world_size, split_size, partitions, shards, devices, pre_trained, logging):
    args = [
        json.dumps(partitions),
        json.dumps(shards),
        json.dumps(devices).replace("\"", '\\"'),
        "--micro_batch_size", str(split_size),
        "--pre_trained", str(int(pre_trained)),
        "--logging", str(int(pre_trained))
    ]
    host = "10.200.103.227"
    host_port = "54389"

    node_list = ["eti-research-dev7"]
    for i, node in enumerate(node_list):
        torchrun_cmd = f"torchrun --nnodes=2 --node-rank={i + 1} --nproc-per-node=3 --rdzv-id=819 --rdzv-backend=c10d --rdzv-endpoint={host}:{host_port} resnet50/resnet50_multinode_inference.py \"{args[0]}\" \"{args[1]}\" \"{args[2]}\" " + " ".join(args[3:])
        cmd = f"ssh changwu@{node} " + "\"" + "source .profile; cd LLM-Inference;" + torchrun_cmd  + "\""
        print("Execute on {}:".format(node), cmd)
        subprocess.Popen(cmd, shell=True)

    torchrun_cmd = f"torchrun --nnodes=2 --node-rank=0 --nproc-per-node=3 --rdzv-id=819 --rdzv-backend=c10d --rdzv-endpoint={host}:{host_port} resnet50/resnet50_multinode_inference.py \"{args[0]}\" \"{args[1]}\" \"{args[2]}\" " + " ".join(args[3:])
    print("Execute on localhost:", torchrun_cmd)
    subprocess.run(torchrun_cmd, shell=True)

if __name__=="__main__":
    with open("resnet50_config.yaml", "r") as config_file:
        config = yaml.safe_load(config_file)
        file = open("./resnet50.log", "w")
        open("./resnet50.csv", "w")
        original_stdout = sys.stdout
        sys.stdout = file

        partitions = config["partitions"]
        devices = config["devices"]
        repeat_times = config["repeat_times"]
        placements = config["placements"]
        split_size = config["micro_batch_size"]
        pre_trained = config["pre_trained"] == "True" if "pre_trained" in config else False
        logging = config["logging"] == "True" if "logging" in config else False
        for shards in placements:
            world_size = len(shards) + 1
            print("Placement:", shards)
            for i in range(repeat_times):
                tik = time.time()
                launch(world_size, split_size, partitions, shards, devices, pre_trained, logging)
                tok = time.time()

                print(f"size of micro-batches = {split_size}, end-to-end execution time = {tok - tik} s")

        sys.stdout = original_stdout